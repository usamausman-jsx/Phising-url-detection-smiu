{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeI_UCpZ1Kfm",
        "outputId": "5c6ff2b2-0bdd-4ea3-af25-70ae59704571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio matplotlib seaborn pandas numpy scikit-learn tensorflow nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import your existing classes and functions\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize NLTK components\n",
        "tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.detector = None\n",
        "        self.models_loaded = False\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805  # Assuming ensemble uses CNN's accuracy\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models\"\"\"\n",
        "        try:\n",
        "            # Load feature extractors\n",
        "            with open('/content/phishing_tfidf_vectorizer.pkl', 'rb') as f:\n",
        "                self.tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "            with open('/content/phishing_feature_extractor.pkl', 'rb') as f:\n",
        "                self.feature_extractor = pickle.load(f)\n",
        "\n",
        "            with open('/content/phishing_keras_tokenizer.pkl', 'rb') as f:\n",
        "                self.keras_tokenizer = pickle.load(f)\n",
        "\n",
        "            # Load ML models\n",
        "            with open('/content/phishing_lr_model.pkl', 'rb') as f:\n",
        "                self.lr_model = pickle.load(f)\n",
        "\n",
        "            with open('/content/phishing_nb_model.pkl', 'rb') as f:\n",
        "                self.nb_model = pickle.load(f)\n",
        "\n",
        "            with open('/content/phishing_rf_model.pkl', 'rb') as f:\n",
        "                self.rf_model = pickle.load(f)\n",
        "\n",
        "            with open('/content/phishing_gb_model.pkl', 'rb') as f:\n",
        "                self.gb_model = pickle.load(f)\n",
        "\n",
        "            # Load deep learning models\n",
        "            self.cnn_model = tf.keras.models.load_model('/content/phishing_cnn_model.keras')\n",
        "            self.lstm_model = tf.keras.models.load_model('/content/phishing_lstm_model.keras')\n",
        "            self.gru_model = tf.keras.models.load_model('/content/phishing_gru_model.keras')\n",
        "            self.hybrid_model = tf.keras.models.load_model('/content/phishing_hybrid_model.keras')\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return \"‚úÖ All models loaded successfully!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        tokens = tokenizer_nltk.tokenize(url_str)\n",
        "        tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                # Combine features for ML models\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "                if model_type == 'lr':\n",
        "                    model = self.lr_model\n",
        "                elif model_type == 'rf':\n",
        "                    model = self.rf_model\n",
        "                elif model_type == 'gb':\n",
        "                    model = self.gb_model\n",
        "\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                # Naive Bayes uses only TF-IDF\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                # Prepare sequence for deep learning\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                if model_type == 'cnn':\n",
        "                    model = self.cnn_model\n",
        "                elif model_type == 'lstm':\n",
        "                    model = self.lstm_model\n",
        "                elif model_type == 'gru':\n",
        "                    model = self.gru_model\n",
        "                elif model_type == 'hybrid':\n",
        "                    model = self.hybrid_model\n",
        "\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            return prediction, proba, features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            # Get predictions from all models\n",
        "            all_probas = []\n",
        "            model_names = ['lr', 'rf', 'gb', 'nb', 'cnn', 'lstm', 'gru', 'hybrid']\n",
        "\n",
        "            # ML models\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            for model_name, model in [('lr', self.lr_model), ('rf', self.rf_model), ('gb', self.gb_model)]:\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(proba)\n",
        "\n",
        "            # Naive Bayes\n",
        "            nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "            all_probas.append(nb_proba)\n",
        "\n",
        "            # Deep learning models\n",
        "            seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "            padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "            for dl_model in [self.cnn_model, self.lstm_model, self.gru_model, self.hybrid_model]:\n",
        "                dl_proba = dl_model.predict(padded, verbose=0)[0][0]\n",
        "                all_probas.append(dl_proba)\n",
        "\n",
        "            # Calculate ensemble average\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            # Create model scores dictionary\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = float(all_probas[i])\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        # Create gradient colors\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Map model abbreviations to full names\n",
        "        model_name_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "        full_names = [model_name_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        # Color based on score (red for phishing, green for legitimate)\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add threshold line\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        # Load models if not already loaded\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                # Map model choice to model type\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                return \"Error in prediction. Please check the URL format.\", None, None, None, None, None\n",
        "\n",
        "            # Calculate probabilities\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            # Create result text\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            # Add key features if available\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            # Create visualizations\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': list(model_scores.keys()),\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            # Create metrics dataframe for display\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "# Create instance\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "# Define CSS for styling\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "    max-width: 1200px !important;\n",
        "    margin: auto !important;\n",
        "}\n",
        ".header {\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    border-radius: 10px;\n",
        "    margin-bottom: 20px;\n",
        "    color: white;\n",
        "}\n",
        ".result-box {\n",
        "    padding: 20px;\n",
        "    border-radius: 10px;\n",
        "    margin: 10px 0;\n",
        "}\n",
        ".phishing-result {\n",
        "    background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);\n",
        "    color: white;\n",
        "}\n",
        ".legitimate-result {\n",
        "    background: linear-gradient(135deg, #51cf66 0%, #40c057 100%);\n",
        "    color: white;\n",
        "}\n",
        ".model-selector {\n",
        "    padding: 15px;\n",
        "    background: #f8f9fa;\n",
        "    border-radius: 10px;\n",
        "    margin: 10px 0;\n",
        "}\n",
        ".footer {\n",
        "    text-align: center;\n",
        "    padding: 10px;\n",
        "    color: #666;\n",
        "    font-size: 12px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(css=css, theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"header\">\n",
        "        <h1>üîó Phishing URL Detection System</h1>\n",
        "        <p>Advanced ML/DL models to detect malicious URLs with high accuracy</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1,\n",
        "                elem_classes=[\"url-input\"]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\",\n",
        "                elem_classes=[\"model-selector\"]\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com/search\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com/user/repository\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php?id=12345\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com/gp/buy\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"footer\">\n",
        "        <p>Phishing URL Detection System | Using Advanced Machine Learning & Deep Learning Models</p>\n",
        "        <p>‚ö†Ô∏è This tool is for educational purposes. Always verify suspicious URLs through official channels.</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Set up event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    # Initialize with model accuracy chart\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    # Load models on startup\n",
        "    detector_ui.load_models()\n",
        "\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=False,\n",
        "        debug=True,\n",
        "        favicon_path=None,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ziZklTy44199",
        "outputId": "6b0ec51e-3238-491e-9d10-11c065b4bc05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2543058888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtokenizer_nltk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[A-Za-z]+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPhishingURLDetectorUI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize NLTK components with error handling\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    # Fallback to basic tokenizer\n",
        "    import re\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.detector = None\n",
        "        self.models_loaded = False\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models\"\"\"\n",
        "        try:\n",
        "            # In Colab, you need to upload or mount your models\n",
        "            import os\n",
        "\n",
        "            # Create saved_models directory if it doesn't exist\n",
        "            if not os.path.exists('/content/'):\n",
        "                os.makedirs('/content/')\n",
        "                return \"‚ö†Ô∏è Created 'saved_models' directory. Please upload your model files here.\"\n",
        "\n",
        "            # List of expected model files\n",
        "            expected_files = [\n",
        "                'phishing_tfidf_vectorizer.pkl',\n",
        "                'phishing_feature_extractor.pkl',\n",
        "                'phishing_keras_tokenizer.pkl',\n",
        "                'phishing_lr_model.pkl',\n",
        "                'phishing_nb_model.pkl',\n",
        "                'phishing_rf_model.pkl',\n",
        "                'phishing_gb_model.pkl'\n",
        "            ]\n",
        "\n",
        "            # List of expected keras files\n",
        "            expected_keras_files = [\n",
        "                'phishing_cnn_model.keras',\n",
        "                'phishing_lstm_model.keras',\n",
        "                'phishing_gru_model.keras',\n",
        "                'phishing_hybrid_model.keras'\n",
        "            ]\n",
        "\n",
        "            # Check which models exist\n",
        "            existing_files = []\n",
        "            for file in expected_files + expected_keras_files:\n",
        "                path = f'/content/{file}'\n",
        "                if os.path.exists(path):\n",
        "                    existing_files.append(file)\n",
        "\n",
        "            if len(existing_files) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found. Please upload your model files to 'saved_models/' directory.\"\n",
        "\n",
        "            print(f\"üìÅ Found {len(existing_files)} model files: {existing_files}\")\n",
        "\n",
        "            # Load pickle models\n",
        "            for file in expected_files:\n",
        "                if file in existing_files:\n",
        "                    with open(f'/content/{file}', 'rb') as f:\n",
        "                        setattr(self, file.replace('.pkl', '').replace('phishing_', ''), pickle.load(f))\n",
        "                        print(f\"‚úÖ Loaded {file}\")\n",
        "\n",
        "            # Load keras models\n",
        "            for file in expected_keras_files:\n",
        "                if file in existing_files:\n",
        "                    model_name = file.replace('.keras', '').replace('phishing_', '')\n",
        "                    setattr(self, f'{model_name}_model', tf.keras.models.load_model(f'saved_models/{file}'))\n",
        "                    print(f\"‚úÖ Loaded {file}\")\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(existing_files)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            # Fallback if NLTK fails\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Check if required models exist\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                # Check if model exists\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Combine features for ML models\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if not hasattr(self, 'nb_model'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Naive Bayes uses only TF-IDF\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                if not hasattr(self, 'keras_tokenizer'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Prepare sequence for deep learning\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            return prediction, proba, features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            # Check required models\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            # Get predictions from all models\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            # ML models\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if hasattr(self, f'{model_name}_model'):\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(proba)\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            # Naive Bayes\n",
        "            if hasattr(self, 'nb_model'):\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(nb_proba)\n",
        "                model_names.append('nb')\n",
        "\n",
        "            # Deep learning models\n",
        "            if hasattr(self, 'keras_tokenizer'):\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    model_attr = f'{model_name}_model'\n",
        "                    if hasattr(self, model_attr):\n",
        "                        model = getattr(self, model_attr)\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(dl_proba)\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Calculate ensemble average\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            # Create model scores dictionary\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = float(all_probas[i])\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        # Create gradient colors\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Map model abbreviations to full names\n",
        "        model_name_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "        full_names = [model_name_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        # Color based on score (red for phishing, green for legitimate)\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add threshold line\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        # Load models if not already loaded\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                # Map model choice to model type\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                return \"Error in prediction. The selected model may not be available.\", None, None, None, None, None\n",
        "\n",
        "            # Calculate probabilities\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            # Create result text\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            # Add key features if available\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            # Create visualizations\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': list(model_scores.keys()),\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            # Create metrics dataframe for display\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "# Create instance\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    # Set up event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    # Initialize with model accuracy chart\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# Launch the app in Colab\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to load models\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "print(\"\\nüìÅ Model files needed in 'saved_models/' directory:\")\n",
        "print(\"1. phishing_tfidf_vectorizer.pkl\")\n",
        "print(\"2. phishing_feature_extractor.pkl\")\n",
        "print(\"3. phishing_keras_tokenizer.pkl\")\n",
        "print(\"4. phishing_lr_model.pkl\")\n",
        "print(\"5. phishing_nb_model.pkl\")\n",
        "print(\"6. phishing_rf_model.pkl\")\n",
        "print(\"7. phishing_gb_model.pkl\")\n",
        "print(\"8. phishing_cnn_model.keras\")\n",
        "print(\"9. phishing_lstm_model.keras\")\n",
        "print(\"10. phishing_gru_model.keras\")\n",
        "print(\"11. phishing_hybrid_model.keras\")\n",
        "print(\"\\nüìù You don't need all files - the system will work with whatever is available.\")\n",
        "\n",
        "# Launch the interface\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "try:\n",
        "    demo.launch(debug=False, share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error launching interface: {e}\")\n",
        "    print(\"\\nTrying without share parameter...\")\n",
        "    demo.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "72T6F7vE5pEp",
        "outputId": "aac907d5-d2a9-4fa3-e3e3-8d9fbeddd5cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading NLTK stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/tmp/ipython-input-2148031607.py:457: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK stopwords downloaded\n",
            "‚úÖ NLTK components initialized successfully\n",
            "üöÄ Launching Phishing URL Detection UI...\n",
            "============================================================\n",
            "üìÅ Found 11 model files: ['phishing_tfidf_vectorizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_keras_tokenizer.pkl', 'phishing_lr_model.pkl', 'phishing_nb_model.pkl', 'phishing_rf_model.pkl', 'phishing_gb_model.pkl', 'phishing_cnn_model.keras', 'phishing_lstm_model.keras', 'phishing_gru_model.keras', 'phishing_hybrid_model.keras']\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "‚ùå Error loading models: Can't get attribute 'EnhancedURLFeatureExtractor' on <module '__main__'>\n",
            "\n",
            "üìÅ Model files needed in 'saved_models/' directory:\n",
            "1. phishing_tfidf_vectorizer.pkl\n",
            "2. phishing_feature_extractor.pkl\n",
            "3. phishing_keras_tokenizer.pkl\n",
            "4. phishing_lr_model.pkl\n",
            "5. phishing_nb_model.pkl\n",
            "6. phishing_rf_model.pkl\n",
            "7. phishing_gb_model.pkl\n",
            "8. phishing_cnn_model.keras\n",
            "9. phishing_lstm_model.keras\n",
            "10. phishing_gru_model.keras\n",
            "11. phishing_hybrid_model.keras\n",
            "\n",
            "üìù You don't need all files - the system will work with whatever is available.\n",
            "\n",
            "üåê Launching Gradio interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1bc1284ca8ea4af3f5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1bc1284ca8ea4af3f5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize NLTK components with error handling\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    # Fallback to basic tokenizer\n",
        "    import re\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.models_loaded = False\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models from root directory\"\"\"\n",
        "        try:\n",
        "            import os\n",
        "\n",
        "            print(\"üìÅ Looking for model files in root directory...\")\n",
        "\n",
        "            # List all files in current directory\n",
        "            files = os.listdir('.')\n",
        "            print(f\"üìã Files in directory: {files}\")\n",
        "\n",
        "            # Check for specific model files\n",
        "            found_models = []\n",
        "\n",
        "            # Check for pickle files\n",
        "            pickle_files = [\n",
        "                'phishing_tfidf_vectorizer.pkl',\n",
        "                'phishing_feature_extractor.pkl',\n",
        "                'phishing_keras_tokenizer.pkl',\n",
        "                'phishing_lr_model.pkl',\n",
        "                'phishing_nb_model.pkl',\n",
        "                'phishing_rf_model.pkl',\n",
        "                'phishing_gb_model.pkl'\n",
        "            ]\n",
        "\n",
        "            # Check for keras files\n",
        "            keras_files = [\n",
        "                'phishing_cnn_model.keras',\n",
        "                'phishing_lstm_model.keras',\n",
        "                'phishing_gru_model.keras',\n",
        "                'phishing_hybrid_model.keras'\n",
        "            ]\n",
        "\n",
        "            # Load pickle models\n",
        "            for file in pickle_files:\n",
        "                if file in files:\n",
        "                    with open(file, 'rb') as f:\n",
        "                        setattr(self, file.replace('.pkl', '').replace('phishing_', ''), pickle.load(f))\n",
        "                        found_models.append(file)\n",
        "                        print(f\"‚úÖ Loaded {file}\")\n",
        "\n",
        "            # Load keras models\n",
        "            for file in keras_files:\n",
        "                if file in files:\n",
        "                    model_name = file.replace('.keras', '').replace('phishing_', '')\n",
        "                    setattr(self, f'{model_name}_model', tf.keras.models.load_model(file))\n",
        "                    found_models.append(file)\n",
        "                    print(f\"‚úÖ Loaded {file}\")\n",
        "\n",
        "            if len(found_models) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found in current directory.\"\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(found_models)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            # Fallback if NLTK fails\n",
        "            import re\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Check if required models exist\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                # Check if model exists\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Combine features for ML models\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if not hasattr(self, 'nb_model'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Naive Bayes uses only TF-IDF\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                if not hasattr(self, 'keras_tokenizer'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Prepare sequence for deep learning\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            return prediction, proba, features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            # Check required models\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            # Get predictions from all models\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            # ML models\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if hasattr(self, f'{model_name}_model'):\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(proba)\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            # Naive Bayes\n",
        "            if hasattr(self, 'nb_model'):\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(nb_proba)\n",
        "                model_names.append('nb')\n",
        "\n",
        "            # Deep learning models\n",
        "            if hasattr(self, 'keras_tokenizer'):\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    model_attr = f'{model_name}_model'\n",
        "                    if hasattr(self, model_attr):\n",
        "                        model = getattr(self, model_attr)\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(dl_proba)\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Calculate ensemble average\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            # Create model scores dictionary\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = float(all_probas[i])\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        # Create gradient colors\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Map model abbreviations to full names\n",
        "        model_name_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "        full_names = [model_name_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        # Color based on score (red for phishing, green for legitimate)\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add threshold line\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        # Load models if not already loaded\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                # Map model choice to model type\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                # If model not found, try demo mode\n",
        "                return self.demo_prediction(url, model_choice), None, None, None, None, \"‚ö†Ô∏è Using demo mode\"\n",
        "\n",
        "            # Calculate probabilities\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            # Create result text\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            # Add key features if available\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            # Create visualizations\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': list(model_scores.keys()),\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            # Create metrics dataframe for display\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "    def demo_prediction(self, url, model_choice):\n",
        "        \"\"\"Demo prediction when models are not available\"\"\"\n",
        "        # Simple heuristic-based prediction for demo\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        # Common phishing indicators\n",
        "        phishing_indicators = ['login', 'verify', 'secure', 'account', 'bank', 'paypal', 'update']\n",
        "        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz']\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        # Check for HTTPS\n",
        "        if not url_lower.startswith('https://'):\n",
        "            score += 0.2\n",
        "\n",
        "        # Check for IP address\n",
        "        import re\n",
        "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_lower):\n",
        "            score += 0.3\n",
        "\n",
        "        # Check for phishing keywords\n",
        "        for keyword in phishing_indicators:\n",
        "            if keyword in url_lower:\n",
        "                score += 0.1\n",
        "\n",
        "        # Check for suspicious TLDs\n",
        "        for tld in suspicious_tlds:\n",
        "            if tld in url_lower:\n",
        "                score += 0.2\n",
        "\n",
        "        # Check URL length\n",
        "        if len(url) > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        # Normalize score\n",
        "        phishing_prob = min(score, 0.9)\n",
        "\n",
        "        result_text = f\"## üîç Analysis Results (DEMO MODE)\\n\\n\"\n",
        "        result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "        result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "        result_text += f\"‚ö†Ô∏è **Note:** Running in demo mode. Models not fully loaded.\\n\\n\"\n",
        "\n",
        "        if phishing_prob > 0.5:\n",
        "            result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "        else:\n",
        "            result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {(1-phishing_prob)*100:.2f}%\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "# Create instance\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    # Set up event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    # Initialize with model accuracy chart\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# Launch the app in Colab\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to load models\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "# List what models were found\n",
        "print(\"\\nüìã Checking for model files...\")\n",
        "import os\n",
        "files = os.listdir('.')\n",
        "print(f\"Files in current directory: {files}\")\n",
        "\n",
        "# Check specifically for CNN model\n",
        "if 'phishing_cnn_model.keras' in files:\n",
        "    print(\"‚úÖ Found: phishing_cnn_model.keras\")\n",
        "else:\n",
        "    print(\"‚ùå Missing: phishing_cnn_model.keras\")\n",
        "    # Try to copy from content if it exists\n",
        "    if os.path.exists('/content/phishing_cnn_model.keras'):\n",
        "        print(\"üìÅ Found CNN model in /content directory\")\n",
        "        import shutil\n",
        "        shutil.copy('/content/phishing_cnn_model.keras', 'phishing_cnn_model.keras')\n",
        "        print(\"‚úÖ Copied phishing_cnn_model.keras to current directory\")\n",
        "\n",
        "print(\"\\nüìù If models are missing, you can:\")\n",
        "print(\"1. Upload model files using the file browser on the left\")\n",
        "print(\"2. Use demo mode (automatic fallback)\")\n",
        "print(\"3. Download sample model files from your original training code\")\n",
        "\n",
        "# Launch the interface\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "try:\n",
        "    demo.launch(debug=True, share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error launching interface: {e}\")\n",
        "    print(\"\\nTrying without share parameter...\")\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HgDoj8B18z2B",
        "outputId": "ab1e9019-551b-48ab-c507-97c6a8cf191f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK stopwords already downloaded\n",
            "‚úÖ NLTK components initialized successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2208108496.py:505: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Phishing URL Detection UI...\n",
            "============================================================\n",
            "üìÅ Looking for model files in root directory...\n",
            "üìã Files in directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "‚ùå Error loading models: Can't get attribute 'EnhancedURLFeatureExtractor' on <module '__main__'>\n",
            "\n",
            "üìã Checking for model files...\n",
            "Files in current directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "‚úÖ Found: phishing_cnn_model.keras\n",
            "\n",
            "üìù If models are missing, you can:\n",
            "1. Upload model files using the file browser on the left\n",
            "2. Use demo mode (automatic fallback)\n",
            "3. Download sample model files from your original training code\n",
            "\n",
            "üåê Launching Gradio interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fc5bafbe6268a377d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fc5bafbe6268a377d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Looking for model files in root directory...\n",
            "üìã Files in directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1bc1284ca8ea4af3f5.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://fc5bafbe6268a377d9.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ========================\n",
        "# DEFINE ENHANCEDURLFEATUREEXTRACTOR CLASS FIRST\n",
        "# ========================\n",
        "class EnhancedURLFeatureExtractor:\n",
        "    \"\"\"Extract comprehensive features from URLs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.phishing_keywords = [\n",
        "            'login', 'signin', 'verify', 'secure', 'account', 'update',\n",
        "            'banking', 'paypal', 'confirm', 'password', 'authenticate',\n",
        "            'validation', 'security', 'webscr', 'signup', 'login-secure',\n",
        "            'bank', 'credit', 'card', 'ssn', 'social', 'irs', 'tax',\n",
        "            'update', 'verify', 'wallet', 'bitcoin', 'crypto', 'wallet'\n",
        "        ]\n",
        "\n",
        "        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz',\n",
        "                                '.top', '.club', '.work', '.online', '.site']\n",
        "\n",
        "        self.shortening_services = ['bit.ly', 'tinyurl', 'goo.gl', 'shorte.st',\n",
        "                                   'ow.ly', 't.co', 'is.gd', 'cli.gs', 'yfrog.com',\n",
        "                                   'migre.me', 'ff.im', 'tiny.cc', 'url4.eu',\n",
        "                                   'twit.ac', 'su.pr', 'twurl.nl', 'snipurl.com',\n",
        "                                   'short.to', 'budurl.com', 'ping.fm', 'post.ly',\n",
        "                                   'just.as', 'bkite.com', 'snipr.com', 'fic.kr',\n",
        "                                   'loopt.us', 'doiop.com', 'short.ie', 'kl.am',\n",
        "                                   'wp.me', 'rubyurl.com', 'om.ly', 'to.ly',\n",
        "                                   'bit.do', 't.co', 'lnkd.in', 'db.tt', 'qr.ae',\n",
        "                                   'adf.ly', 'goo.gl', 'bitly.com', 'cur.lv',\n",
        "                                   'tinyurl.com', 'ow.ly', 'bit.ly', 'ity.im',\n",
        "                                   'q.gs', 'is.gd', 'po.st', 'bc.vc', 'twitthis.com',\n",
        "                                   'u.to', 'j.mp', 'buzurl.com', 'cutt.us',\n",
        "                                   'u.bb', 'yourls.org', 'x.co', 'prettylinkpro.com',\n",
        "                                   'scrnch.me', 'filoops.info', 'vzturl.com',\n",
        "                                   'qr.net', '1url.com', 'tweez.me', 'v.gd',\n",
        "                                   'tr.im', 'link.zip.net']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        features = {}\n",
        "\n",
        "        # URL string\n",
        "        url_str = str(url).lower()\n",
        "\n",
        "        # 1. Length-based features\n",
        "        features['url_length'] = len(url_str)\n",
        "        features['hostname_length'] = len(url_str.split('//')[-1].split('/')[0]) if '//' in url_str else len(url_str.split('/')[0])\n",
        "        features['path_length'] = len('/'.join(url_str.split('/')[3:]))\n",
        "        features['num_dots'] = url_str.count('.')\n",
        "        features['num_hyphens'] = url_str.count('-')\n",
        "        features['num_underscores'] = url_str.count('_')\n",
        "        features['num_slashes'] = url_str.count('/')\n",
        "        features['num_questionmarks'] = url_str.count('?')\n",
        "        features['num_equals'] = url_str.count('=')\n",
        "        features['num_ats'] = url_str.count('@')\n",
        "        features['num_ampersands'] = url_str.count('&')\n",
        "        features['num_percent'] = url_str.count('%')\n",
        "\n",
        "        # 2. Protocol features\n",
        "        features['has_https'] = 1 if url_str.startswith('https://') else 0\n",
        "        features['has_http'] = 1 if url_str.startswith('http://') else 0\n",
        "\n",
        "        # 3. Domain features\n",
        "        if '//' in url_str:\n",
        "            domain_part = url_str.split('//')[1].split('/')[0]\n",
        "        else:\n",
        "            domain_part = url_str.split('/')[0]\n",
        "\n",
        "        features['domain_length'] = len(domain_part)\n",
        "        features['num_subdomains'] = domain_part.count('.') - 1 if '.' in domain_part else 0\n",
        "\n",
        "        # 4. TLD features\n",
        "        tld = domain_part.split('.')[-1] if '.' in domain_part else ''\n",
        "        features['has_suspicious_tld'] = 1 if any(suspicious_tld in url_str for suspicious_tld in self.suspicious_tlds) else 0\n",
        "        features['tld_length'] = len(tld)\n",
        "\n",
        "        # 5. URL shortening detection\n",
        "        features['is_shortened'] = 1 if any(short in domain_part for short in self.shortening_services) else 0\n",
        "\n",
        "        # 6. Keyword features\n",
        "        keyword_count = 0\n",
        "        for keyword in self.phishing_keywords:\n",
        "            if keyword in url_str:\n",
        "                keyword_count += 1\n",
        "\n",
        "        features['phishing_keyword_count'] = keyword_count\n",
        "        features['has_phishing_keyword'] = 1 if keyword_count > 0 else 0\n",
        "\n",
        "        # 7. Suspicious patterns\n",
        "        features['has_ip'] = 1 if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_str) else 0\n",
        "        features['hex_chars_ratio'] = sum(1 for c in url_str if c in '0123456789abcdef') / max(len(url_str), 1)\n",
        "\n",
        "        # 8. Character distribution features\n",
        "        features['digit_ratio'] = sum(1 for c in url_str if c.isdigit()) / max(len(url_str), 1)\n",
        "        features['letter_ratio'] = sum(1 for c in url_str if c.isalpha()) / max(len(url_str), 1)\n",
        "        features['special_char_ratio'] = sum(1 for c in url_str if not c.isalnum() and c not in ['.', '-', '_', '/']) / max(len(url_str), 1)\n",
        "        features['vowel_ratio'] = sum(1 for c in url_str if c in 'aeiou') / max(len(url_str), 1)\n",
        "\n",
        "        # 9. Specific pattern features\n",
        "        features['has_login'] = 1 if 'login' in url_str else 0\n",
        "        features['has_signin'] = 1 if 'signin' in url_str else 0\n",
        "        features['has_verify'] = 1 if 'verify' in url_str else 0\n",
        "        features['has_bank'] = 1 if 'bank' in url_str else 0\n",
        "        features['has_paypal'] = 1 if 'paypal' in url_str else 0\n",
        "        features['has_secure'] = 1 if 'secure' in url_str else 0\n",
        "\n",
        "        # 10. Entropy (measure of randomness)\n",
        "        if url_str:\n",
        "            freq = Counter(url_str)\n",
        "            prob = [float(freq[c]) / len(url_str) for c in freq]\n",
        "            features['entropy'] = -sum([p * math.log(p) / math.log(2.0) for p in prob])\n",
        "        else:\n",
        "            features['entropy'] = 0\n",
        "\n",
        "        # 11. Consecutive characters\n",
        "        features['consecutive_digits'] = max(len(match) for match in re.findall(r'\\d+', url_str)) if re.findall(r'\\d+', url_str) else 0\n",
        "        features['consecutive_chars'] = max(len(match) for match in re.findall(r'[a-z]+', url_str)) if re.findall(r'[a-z]+', url_str) else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def transform(self, urls):\n",
        "        features_list = []\n",
        "        for url in urls:\n",
        "            features = self.extract_features(url)\n",
        "            features_list.append(list(features.values()))\n",
        "\n",
        "        feature_names = list(self.extract_features(\"https://example.com\").keys())\n",
        "        return pd.DataFrame(features_list, columns=feature_names)\n",
        "\n",
        "# ========================\n",
        "# NOW CONTINUE WITH THE REST\n",
        "# ========================\n",
        "\n",
        "# Initialize NLTK components with error handling\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    # Fallback to basic tokenizer\n",
        "    import re\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.models_loaded = False\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models from root directory\"\"\"\n",
        "        try:\n",
        "            import os\n",
        "\n",
        "            print(\"üìÅ Looking for model files in root directory...\")\n",
        "\n",
        "            # List all files in current directory\n",
        "            files = os.listdir('.')\n",
        "            print(f\"üìã Files in directory: {files}\")\n",
        "\n",
        "            # Check for specific model files\n",
        "            found_models = []\n",
        "\n",
        "            # Check for pickle files\n",
        "            pickle_files = [\n",
        "                'phishing_tfidf_vectorizer.pkl',\n",
        "                'phishing_feature_extractor.pkl',\n",
        "                'phishing_keras_tokenizer.pkl',\n",
        "                'phishing_lr_model.pkl',\n",
        "                'phishing_nb_model.pkl',\n",
        "                'phishing_rf_model.pkl',\n",
        "                'phishing_gb_model.pkl'\n",
        "            ]\n",
        "\n",
        "            # Check for keras files\n",
        "            keras_files = [\n",
        "                'phishing_cnn_model.keras',\n",
        "                'phishing_lstm_model.keras',\n",
        "                'phishing_gru_model.keras',\n",
        "                'phishing_hybrid_model.keras',\n",
        "                'best_cnn_model.keras',\n",
        "                'best_lstm_model.keras',\n",
        "                'best_gru_model.keras',\n",
        "                'best_hybrid_model.keras'\n",
        "            ]\n",
        "\n",
        "            # Load pickle models\n",
        "            for file in pickle_files:\n",
        "                if file in files:\n",
        "                    try:\n",
        "                        with open(file, 'rb') as f:\n",
        "                            setattr(self, file.replace('.pkl', '').replace('phishing_', ''), pickle.load(f))\n",
        "                            found_models.append(file)\n",
        "                            print(f\"‚úÖ Loaded {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file}: {e}\")\n",
        "\n",
        "            # Load keras models\n",
        "            for file in keras_files:\n",
        "                if file in files:\n",
        "                    try:\n",
        "                        model_name = file.replace('.keras', '').replace('phishing_', '').replace('best_', '')\n",
        "                        setattr(self, f'{model_name}_model', tf.keras.models.load_model(file))\n",
        "                        found_models.append(file)\n",
        "                        print(f\"‚úÖ Loaded {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file}: {e}\")\n",
        "\n",
        "            if len(found_models) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found in current directory.\"\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(found_models)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            # Fallback if NLTK fails\n",
        "            import re\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Check if required models exist\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                # Check if model exists\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Combine features for ML models\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if not hasattr(self, 'nb_model'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Naive Bayes uses only TF-IDF\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                if not hasattr(self, 'keras_tokenizer'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Prepare sequence for deep learning\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            return prediction, proba, features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            # Check required models\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            # Get predictions from all models\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            # ML models\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if hasattr(self, f'{model_name}_model'):\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(proba)\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            # Naive Bayes\n",
        "            if hasattr(self, 'nb_model'):\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(nb_proba)\n",
        "                model_names.append('nb')\n",
        "\n",
        "            # Deep learning models\n",
        "            if hasattr(self, 'keras_tokenizer'):\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    model_attr = f'{model_name}_model'\n",
        "                    if hasattr(self, model_attr):\n",
        "                        model = getattr(self, model_attr)\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(dl_proba)\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Calculate ensemble average\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            # Create model scores dictionary\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = float(all_probas[i])\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        # Create gradient colors\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Map model abbreviations to full names\n",
        "        model_name_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "        full_names = [model_name_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        # Color based on score (red for phishing, green for legitimate)\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add threshold line\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        # Load models if not already loaded\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                # Map model choice to model type\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                # If model not found, try demo mode\n",
        "                return self.demo_prediction(url, model_choice), None, None, None, None, \"‚ö†Ô∏è Using demo mode\"\n",
        "\n",
        "            # Calculate probabilities\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            # Create result text\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            # Add key features if available\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            # Create visualizations\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': list(model_scores.keys()),\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            # Create metrics dataframe for display\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "    def demo_prediction(self, url, model_choice):\n",
        "        \"\"\"Demo prediction when models are not available\"\"\"\n",
        "        # Simple heuristic-based prediction for demo\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        # Common phishing indicators\n",
        "        phishing_indicators = ['login', 'verify', 'secure', 'account', 'bank', 'paypal', 'update']\n",
        "        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz']\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        # Check for HTTPS\n",
        "        if not url_lower.startswith('https://'):\n",
        "            score += 0.2\n",
        "\n",
        "        # Check for IP address\n",
        "        import re\n",
        "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_lower):\n",
        "            score += 0.3\n",
        "\n",
        "        # Check for phishing keywords\n",
        "        for keyword in phishing_indicators:\n",
        "            if keyword in url_lower:\n",
        "                score += 0.1\n",
        "\n",
        "        # Check for suspicious TLDs\n",
        "        for tld in suspicious_tlds:\n",
        "            if tld in url_lower:\n",
        "                score += 0.2\n",
        "\n",
        "        # Check URL length\n",
        "        if len(url) > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        # Normalize score\n",
        "        phishing_prob = min(score, 0.9)\n",
        "\n",
        "        result_text = f\"## üîç Analysis Results (DEMO MODE)\\n\\n\"\n",
        "        result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "        result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "        result_text += f\"‚ö†Ô∏è **Note:** Running in demo mode. Models not fully loaded.\\n\\n\"\n",
        "\n",
        "        if phishing_prob > 0.5:\n",
        "            result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "        else:\n",
        "            result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {(1-phishing_prob)*100:.2f}%\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "# Create instance\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    # Set up event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    # Initialize with model accuracy chart\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# Launch the app in Colab\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to load models\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "# List what models were found\n",
        "print(\"\\nüìã Checking for model files...\")\n",
        "import os\n",
        "files = os.listdir('.')\n",
        "print(f\"Files in current directory: {files}\")\n",
        "\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, theme=gr.themes.Soft())\n",
        "except Exception as e:\n",
        "    print(f\"Error launching interface: {e}\")\n",
        "    print(\"\\nTrying without share parameter...\")\n",
        "    demo.launch(debug=True, theme=gr.themes.Soft())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "id": "CaN2Pugz9fif",
        "outputId": "0295898a-1081-493b-926c-1387ee3346d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK stopwords already downloaded\n",
            "‚úÖ NLTK components initialized successfully\n",
            "üöÄ Launching Phishing URL Detection UI...\n",
            "============================================================\n",
            "üìÅ Looking for model files in root directory...\n",
            "üìã Files in directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "‚úÖ Loaded phishing_feature_extractor.pkl\n",
            "‚úÖ Loaded phishing_keras_tokenizer.pkl\n",
            "‚úÖ Loaded phishing_lr_model.pkl\n",
            "‚úÖ Loaded phishing_nb_model.pkl\n",
            "‚úÖ Loaded phishing_rf_model.pkl\n",
            "‚úÖ Loaded phishing_gb_model.pkl\n",
            "‚úÖ Loaded phishing_cnn_model.keras\n",
            "‚úÖ Loaded phishing_lstm_model.keras\n",
            "‚úÖ Loaded phishing_gru_model.keras\n",
            "‚úÖ Loaded phishing_hybrid_model.keras\n",
            "‚úÖ Loaded best_cnn_model.keras\n",
            "‚úÖ Loaded best_lstm_model.keras\n",
            "‚úÖ Loaded best_gru_model.keras\n",
            "‚úÖ Loaded best_hybrid_model.keras\n",
            "‚úÖ Successfully loaded 15 model files!\n",
            "\n",
            "üìã Checking for model files...\n",
            "Files in current directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "\n",
            "üåê Launching Gradio interface...\n",
            "Error launching interface: Blocks.launch() got an unexpected keyword argument 'theme'\n",
            "\n",
            "Trying without share parameter...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Blocks.launch() got an unexpected keyword argument 'theme'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3607986392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Blocks.launch() got an unexpected keyword argument 'theme'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3607986392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error launching interface: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTrying without share parameter...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Blocks.launch() got an unexpected keyword argument 'theme'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ========================\n",
        "# DEFINE ENHANCEDURLFEATUREEXTRACTOR CLASS FIRST\n",
        "# ========================\n",
        "class EnhancedURLFeatureExtractor:\n",
        "    \"\"\"Extract comprehensive features from URLs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.phishing_keywords = [\n",
        "            'login', 'signin', 'verify', 'secure', 'account', 'update',\n",
        "            'banking', 'paypal', 'confirm', 'password', 'authenticate',\n",
        "            'validation', 'security', 'webscr', 'signup', 'login-secure',\n",
        "            'bank', 'credit', 'card', 'ssn', 'social', 'irs', 'tax',\n",
        "            'update', 'verify', 'wallet', 'bitcoin', 'crypto', 'wallet'\n",
        "        ]\n",
        "\n",
        "        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz',\n",
        "                                '.top', '.club', '.work', '.online', '.site']\n",
        "\n",
        "        self.shortening_services = ['bit.ly', 'tinyurl', 'goo.gl', 'shorte.st',\n",
        "                                   'ow.ly', 't.co', 'is.gd', 'cli.gs', 'yfrog.com',\n",
        "                                   'migre.me', 'ff.im', 'tiny.cc', 'url4.eu',\n",
        "                                   'twit.ac', 'su.pr', 'twurl.nl', 'snipurl.com',\n",
        "                                   'short.to', 'budurl.com', 'ping.fm', 'post.ly',\n",
        "                                   'just.as', 'bkite.com', 'snipr.com', 'fic.kr',\n",
        "                                   'loopt.us', 'doiop.com', 'short.ie', 'kl.am',\n",
        "                                   'wp.me', 'rubyurl.com', 'om.ly', 'to.ly',\n",
        "                                   'bit.do', 't.co', 'lnkd.in', 'db.tt', 'qr.ae',\n",
        "                                   'adf.ly', 'goo.gl', 'bitly.com', 'cur.lv',\n",
        "                                   'tinyurl.com', 'ow.ly', 'bit.ly', 'ity.im',\n",
        "                                   'q.gs', 'is.gd', 'po.st', 'bc.vc', 'twitthis.com',\n",
        "                                   'u.to', 'j.mp', 'buzurl.com', 'cutt.us',\n",
        "                                   'u.bb', 'yourls.org', 'x.co', 'prettylinkpro.com',\n",
        "                                   'scrnch.me', 'filoops.info', 'vzturl.com',\n",
        "                                   'qr.net', '1url.com', 'tweez.me', 'v.gd',\n",
        "                                   'tr.im', 'link.zip.net']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        features = {}\n",
        "\n",
        "        # URL string\n",
        "        url_str = str(url).lower()\n",
        "\n",
        "        # 1. Length-based features\n",
        "        features['url_length'] = len(url_str)\n",
        "        features['hostname_length'] = len(url_str.split('//')[-1].split('/')[0]) if '//' in url_str else len(url_str.split('/')[0])\n",
        "        features['path_length'] = len('/'.join(url_str.split('/')[3:]))\n",
        "        features['num_dots'] = url_str.count('.')\n",
        "        features['num_hyphens'] = url_str.count('-')\n",
        "        features['num_underscores'] = url_str.count('_')\n",
        "        features['num_slashes'] = url_str.count('/')\n",
        "        features['num_questionmarks'] = url_str.count('?')\n",
        "        features['num_equals'] = url_str.count('=')\n",
        "        features['num_ats'] = url_str.count('@')\n",
        "        features['num_ampersands'] = url_str.count('&')\n",
        "        features['num_percent'] = url_str.count('%')\n",
        "\n",
        "        # 2. Protocol features\n",
        "        features['has_https'] = 1 if url_str.startswith('https://') else 0\n",
        "        features['has_http'] = 1 if url_str.startswith('http://') else 0\n",
        "\n",
        "        # 3. Domain features\n",
        "        if '//' in url_str:\n",
        "            domain_part = url_str.split('//')[1].split('/')[0]\n",
        "        else:\n",
        "            domain_part = url_str.split('/')[0]\n",
        "\n",
        "        features['domain_length'] = len(domain_part)\n",
        "        features['num_subdomains'] = domain_part.count('.') - 1 if '.' in domain_part else 0\n",
        "\n",
        "        # 4. TLD features\n",
        "        tld = domain_part.split('.')[-1] if '.' in domain_part else ''\n",
        "        features['has_suspicious_tld'] = 1 if any(suspicious_tld in url_str for suspicious_tld in self.suspicious_tlds) else 0\n",
        "        features['tld_length'] = len(tld)\n",
        "\n",
        "        # 5. URL shortening detection\n",
        "        features['is_shortened'] = 1 if any(short in domain_part for short in self.shortening_services) else 0\n",
        "\n",
        "        # 6. Keyword features\n",
        "        keyword_count = 0\n",
        "        for keyword in self.phishing_keywords:\n",
        "            if keyword in url_str:\n",
        "                keyword_count += 1\n",
        "\n",
        "        features['phishing_keyword_count'] = keyword_count\n",
        "        features['has_phishing_keyword'] = 1 if keyword_count > 0 else 0\n",
        "\n",
        "        # 7. Suspicious patterns\n",
        "        features['has_ip'] = 1 if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_str) else 0\n",
        "        features['hex_chars_ratio'] = sum(1 for c in url_str if c in '0123456789abcdef') / max(len(url_str), 1)\n",
        "\n",
        "        # 8. Character distribution features\n",
        "        features['digit_ratio'] = sum(1 for c in url_str if c.isdigit()) / max(len(url_str), 1)\n",
        "        features['letter_ratio'] = sum(1 for c in url_str if c.isalpha()) / max(len(url_str), 1)\n",
        "        features['special_char_ratio'] = sum(1 for c in url_str if not c.isalnum() and c not in ['.', '-', '_', '/']) / max(len(url_str), 1)\n",
        "        features['vowel_ratio'] = sum(1 for c in url_str if c in 'aeiou') / max(len(url_str), 1)\n",
        "\n",
        "        # 9. Specific pattern features\n",
        "        features['has_login'] = 1 if 'login' in url_str else 0\n",
        "        features['has_signin'] = 1 if 'signin' in url_str else 0\n",
        "        features['has_verify'] = 1 if 'verify' in url_str else 0\n",
        "        features['has_bank'] = 1 if 'bank' in url_str else 0\n",
        "        features['has_paypal'] = 1 if 'paypal' in url_str else 0\n",
        "        features['has_secure'] = 1 if 'secure' in url_str else 0\n",
        "\n",
        "        # 10. Entropy (measure of randomness)\n",
        "        if url_str:\n",
        "            freq = Counter(url_str)\n",
        "            prob = [float(freq[c]) / len(url_str) for c in freq]\n",
        "            features['entropy'] = -sum([p * math.log(p) / math.log(2.0) for p in prob])\n",
        "        else:\n",
        "            features['entropy'] = 0\n",
        "\n",
        "        # 11. Consecutive characters\n",
        "        features['consecutive_digits'] = max(len(match) for match in re.findall(r'\\d+', url_str)) if re.findall(r'\\d+', url_str) else 0\n",
        "        features['consecutive_chars'] = max(len(match) for match in re.findall(r'[a-z]+', url_str)) if re.findall(r'[a-z]+', url_str) else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def transform(self, urls):\n",
        "        features_list = []\n",
        "        for url in urls:\n",
        "            features = self.extract_features(url)\n",
        "            features_list.append(list(features.values()))\n",
        "\n",
        "        feature_names = list(self.extract_features(\"https://example.com\").keys())\n",
        "        return pd.DataFrame(features_list, columns=feature_names)\n",
        "\n",
        "# ========================\n",
        "# NOW CONTINUE WITH THE REST\n",
        "# ========================\n",
        "\n",
        "# Initialize NLTK components with error handling\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    # Fallback to basic tokenizer\n",
        "    import re\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.models_loaded = False\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models from root directory\"\"\"\n",
        "        try:\n",
        "            import os\n",
        "\n",
        "            print(\"üìÅ Looking for model files in root directory...\")\n",
        "\n",
        "            # List all files in current directory\n",
        "            files = os.listdir('.')\n",
        "            print(f\"üìã Files in directory: {files}\")\n",
        "\n",
        "            # Check for specific model files\n",
        "            found_models = []\n",
        "\n",
        "            # Check for pickle files\n",
        "            pickle_files = [\n",
        "                'phishing_tfidf_vectorizer.pkl',\n",
        "                'phishing_feature_extractor.pkl',\n",
        "                'phishing_keras_tokenizer.pkl',\n",
        "                'phishing_lr_model.pkl',\n",
        "                'phishing_nb_model.pkl',\n",
        "                'phishing_rf_model.pkl',\n",
        "                'phishing_gb_model.pkl'\n",
        "            ]\n",
        "\n",
        "            # Check for keras files\n",
        "            keras_files = [\n",
        "                'phishing_cnn_model.keras',\n",
        "                'phishing_lstm_model.keras',\n",
        "                'phishing_gru_model.keras',\n",
        "                'phishing_hybrid_model.keras',\n",
        "                'best_cnn_model.keras',\n",
        "                'best_lstm_model.keras',\n",
        "                'best_gru_model.keras',\n",
        "                'best_hybrid_model.keras'\n",
        "            ]\n",
        "\n",
        "            # Load pickle models\n",
        "            for file in pickle_files:\n",
        "                if file in files:\n",
        "                    try:\n",
        "                        with open(file, 'rb') as f:\n",
        "                            setattr(self, file.replace('.pkl', '').replace('phishing_', ''), pickle.load(f))\n",
        "                            found_models.append(file)\n",
        "                            print(f\"‚úÖ Loaded {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file}: {e}\")\n",
        "\n",
        "            # Load keras models\n",
        "            for file in keras_files:\n",
        "                if file in files:\n",
        "                    try:\n",
        "                        model_name = file.replace('.keras', '').replace('phishing_', '').replace('best_', '')\n",
        "                        setattr(self, f'{model_name}_model', tf.keras.models.load_model(file))\n",
        "                        found_models.append(file)\n",
        "                        print(f\"‚úÖ Loaded {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file}: {e}\")\n",
        "\n",
        "            if len(found_models) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found in current directory.\"\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(found_models)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            # Fallback if NLTK fails\n",
        "            import re\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Check if required models exist\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                # Check if model exists\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Combine features for ML models\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if not hasattr(self, 'nb_model'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Naive Bayes uses only TF-IDF\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if not hasattr(self, model_attr):\n",
        "                    return None, None, None\n",
        "\n",
        "                if not hasattr(self, 'keras_tokenizer'):\n",
        "                    return None, None, None\n",
        "\n",
        "                # Prepare sequence for deep learning\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            return prediction, proba, features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            # Check required models\n",
        "            if not hasattr(self, 'feature_extractor') or not hasattr(self, 'tfidf_vectorizer'):\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Preprocess URL\n",
        "            processed_url = self.preprocess_url(url)\n",
        "\n",
        "            # Extract handcrafted features\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # TF-IDF features\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            # Get predictions from all models\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            # ML models\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if hasattr(self, f'{model_name}_model'):\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(proba)\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            # Naive Bayes\n",
        "            if hasattr(self, 'nb_model'):\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(nb_proba)\n",
        "                model_names.append('nb')\n",
        "\n",
        "            # Deep learning models\n",
        "            if hasattr(self, 'keras_tokenizer'):\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    model_attr = f'{model_name}_model'\n",
        "                    if hasattr(self, model_attr):\n",
        "                        model = getattr(self, model_attr)\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(dl_proba)\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Calculate ensemble average\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            # Create model scores dictionary\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = float(all_probas[i])\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        # Create gradient colors\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Map model abbreviations to full names\n",
        "        model_name_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "        full_names = [model_name_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        # Color based on score (red for phishing, green for legitimate)\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add threshold line\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        # Load models if not already loaded\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                # Map model choice to model type\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                # If model not found, try demo mode\n",
        "                return self.demo_prediction(url, model_choice), None, None, None, None, \"‚ö†Ô∏è Using demo mode\"\n",
        "\n",
        "            # Calculate probabilities\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            # Create result text\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            # Add key features if available\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            # Create visualizations\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': list(model_scores.keys()),\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            # Create metrics dataframe for display\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "    def demo_prediction(self, url, model_choice):\n",
        "        \"\"\"Demo prediction when models are not available\"\"\"\n",
        "        # Simple heuristic-based prediction for demo\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        # Common phishing indicators\n",
        "        phishing_indicators = ['login', 'verify', 'secure', 'account', 'bank', 'paypal', 'update']\n",
        "        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz']\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        # Check for HTTPS\n",
        "        if not url_lower.startswith('https://'):\n",
        "            score += 0.2\n",
        "\n",
        "        # Check for IP address\n",
        "        import re\n",
        "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_lower):\n",
        "            score += 0.3\n",
        "\n",
        "        # Check for phishing keywords\n",
        "        for keyword in phishing_indicators:\n",
        "            if keyword in url_lower:\n",
        "                score += 0.1\n",
        "\n",
        "        # Check for suspicious TLDs\n",
        "        for tld in suspicious_tlds:\n",
        "            if tld in url_lower:\n",
        "                score += 0.2\n",
        "\n",
        "        # Check URL length\n",
        "        if len(url) > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        # Normalize score\n",
        "        phishing_prob = min(score, 0.9)\n",
        "\n",
        "        result_text = f\"## üîç Analysis Results (DEMO MODE)\\n\\n\"\n",
        "        result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "        result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "        result_text += f\"‚ö†Ô∏è **Note:** Running in demo mode. Models not fully loaded.\\n\\n\"\n",
        "\n",
        "        if phishing_prob > 0.5:\n",
        "            result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "        else:\n",
        "            result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {(1-phishing_prob)*100:.2f}%\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "# Create instance\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "# Create Gradio interface\n",
        "# ... (All the previous code remains the same until the Gradio interface section) ...\n",
        "\n",
        "# Create Gradio interface - FIXED VERSION\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    # Set up event handlers\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    # Initialize with model accuracy chart\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# Launch the app in Colab\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to load models\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "print(\"\\nüìã Models loaded successfully! Now launching interface...\")\n",
        "\n",
        "# Launch the interface - FIXED VERSION (no theme parameter in launch())\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "try:\n",
        "    demo.launch(debug=True, share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error launching interface: {e}\")\n",
        "    print(\"\\nTrying without share parameter...\")\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b12_ISsN-jPg",
        "outputId": "cb390f97-95c0-4ec4-b443-985b10d6d9bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK stopwords already downloaded\n",
            "‚úÖ NLTK components initialized successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3972969296.py:652: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Phishing URL Detection UI...\n",
            "============================================================\n",
            "üìÅ Looking for model files in root directory...\n",
            "üìã Files in directory: ['.config', 'phishing_gb_model.pkl', 'best_gru_model.keras', 'phishing_hybrid_model.keras', 'best_lstm_model.keras', 'phishing_rf_model.pkl', 'phishing_nb_model.pkl', '.gradio', 'phishing_gru_model.keras', 'phishing_site_urls.csv', 'phishing_lr_model.pkl', 'phishing_lstm_model.keras', 'phishing_URL.ipynb', 'phishing_keras_tokenizer.pkl', 'phishing_feature_extractor.pkl', 'phishing_cnn_model.keras', 'best_hybrid_model.keras', 'best_cnn_model.keras', 'phishing_tfidf_vectorizer.pkl', 'sample_data']\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "‚úÖ Loaded phishing_feature_extractor.pkl\n",
            "‚úÖ Loaded phishing_keras_tokenizer.pkl\n",
            "‚úÖ Loaded phishing_lr_model.pkl\n",
            "‚úÖ Loaded phishing_nb_model.pkl\n",
            "‚úÖ Loaded phishing_rf_model.pkl\n",
            "‚úÖ Loaded phishing_gb_model.pkl\n",
            "‚úÖ Loaded phishing_cnn_model.keras\n",
            "‚úÖ Loaded phishing_lstm_model.keras\n",
            "‚úÖ Loaded phishing_gru_model.keras\n",
            "‚úÖ Loaded phishing_hybrid_model.keras\n",
            "‚úÖ Loaded best_cnn_model.keras\n",
            "‚úÖ Loaded best_lstm_model.keras\n",
            "‚úÖ Loaded best_gru_model.keras\n",
            "‚úÖ Loaded best_hybrid_model.keras\n",
            "‚úÖ Successfully loaded 15 model files!\n",
            "\n",
            "üìã Models loaded successfully! Now launching interface...\n",
            "\n",
            "üåê Launching Gradio interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ad982c621663b4d84f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ad982c621663b4d84f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://ad982c621663b4d84f.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ========================\n",
        "# DEFINE ENHANCEDURLFEATUREEXTRACTOR CLASS FIRST\n",
        "# ========================\n",
        "class EnhancedURLFeatureExtractor:\n",
        "    \"\"\"Extract comprehensive features from URLs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.phishing_keywords = [\n",
        "            'login', 'signin', 'verify', 'secure', 'account', 'update',\n",
        "            'banking', 'paypal', 'confirm', 'password', 'authenticate',\n",
        "            'validation', 'security', 'webscr', 'signup', 'login-secure',\n",
        "            'bank', 'credit', 'card', 'ssn', 'social', 'irs', 'tax',\n",
        "            'update', 'verify', 'wallet', 'bitcoin', 'crypto', 'wallet'\n",
        "        ]\n",
        "\n",
        "        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz',\n",
        "                                '.top', '.club', '.work', '.online', '.site']\n",
        "\n",
        "        self.shortening_services = ['bit.ly', 'tinyurl', 'goo.gl', 'shorte.st',\n",
        "                                   'ow.ly', 't.co', 'is.gd', 'cli.gs', 'yfrog.com',\n",
        "                                   'migre.me', 'ff.im', 'tiny.cc', 'url4.eu',\n",
        "                                   'twit.ac', 'su.pr', 'twurl.nl', 'snipurl.com',\n",
        "                                   'short.to', 'budurl.com', 'ping.fm', 'post.ly',\n",
        "                                   'just.as', 'bkite.com', 'snipr.com', 'fic.kr',\n",
        "                                   'loopt.us', 'doiop.com', 'short.ie', 'kl.am',\n",
        "                                   'wp.me', 'rubyurl.com', 'om.ly', 'to.ly',\n",
        "                                   'bit.do', 't.co', 'lnkd.in', 'db.tt', 'qr.ae',\n",
        "                                   'adf.ly', 'goo.gl', 'bitly.com', 'cur.lv',\n",
        "                                   'tinyurl.com', 'ow.ly', 'bit.ly', 'ity.im',\n",
        "                                   'q.gs', 'is.gd', 'po.st', 'bc.vc', 'twitthis.com',\n",
        "                                   'u.to', 'j.mp', 'buzurl.com', 'cutt.us',\n",
        "                                   'u.bb', 'yourls.org', 'x.co', 'prettylinkpro.com',\n",
        "                                   'scrnch.me', 'filoops.info', 'vzturl.com',\n",
        "                                   'qr.net', '1url.com', 'tweez.me', 'v.gd',\n",
        "                                   'tr.im', 'link.zip.net']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        features = {}\n",
        "\n",
        "        # URL string\n",
        "        url_str = str(url).lower()\n",
        "\n",
        "        # 1. Length-based features\n",
        "        features['url_length'] = len(url_str)\n",
        "        features['hostname_length'] = len(url_str.split('//')[-1].split('/')[0]) if '//' in url_str else len(url_str.split('/')[0])\n",
        "        features['path_length'] = len('/'.join(url_str.split('/')[3:]))\n",
        "        features['num_dots'] = url_str.count('.')\n",
        "        features['num_hyphens'] = url_str.count('-')\n",
        "        features['num_underscores'] = url_str.count('_')\n",
        "        features['num_slashes'] = url_str.count('/')\n",
        "        features['num_questionmarks'] = url_str.count('?')\n",
        "        features['num_equals'] = url_str.count('=')\n",
        "        features['num_ats'] = url_str.count('@')\n",
        "        features['num_ampersands'] = url_str.count('&')\n",
        "        features['num_percent'] = url_str.count('%')\n",
        "\n",
        "        # 2. Protocol features\n",
        "        features['has_https'] = 1 if url_str.startswith('https://') else 0\n",
        "        features['has_http'] = 1 if url_str.startswith('http://') else 0\n",
        "\n",
        "        # 3. Domain features\n",
        "        if '//' in url_str:\n",
        "            domain_part = url_str.split('//')[1].split('/')[0]\n",
        "        else:\n",
        "            domain_part = url_str.split('/')[0]\n",
        "\n",
        "        features['domain_length'] = len(domain_part)\n",
        "        features['num_subdomains'] = domain_part.count('.') - 1 if '.' in domain_part else 0\n",
        "\n",
        "        # 4. TLD features\n",
        "        tld = domain_part.split('.')[-1] if '.' in domain_part else ''\n",
        "        features['has_suspicious_tld'] = 1 if any(suspicious_tld in url_str for suspicious_tld in self.suspicious_tlds) else 0\n",
        "        features['tld_length'] = len(tld)\n",
        "\n",
        "        # 5. URL shortening detection\n",
        "        features['is_shortened'] = 1 if any(short in domain_part for short in self.shortening_services) else 0\n",
        "\n",
        "        # 6. Keyword features\n",
        "        keyword_count = 0\n",
        "        for keyword in self.phishing_keywords:\n",
        "            if keyword in url_str:\n",
        "                keyword_count += 1\n",
        "\n",
        "        features['phishing_keyword_count'] = keyword_count\n",
        "        features['has_phishing_keyword'] = 1 if keyword_count > 0 else 0\n",
        "\n",
        "        # 7. Suspicious patterns\n",
        "        features['has_ip'] = 1 if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_str) else 0\n",
        "        features['hex_chars_ratio'] = sum(1 for c in url_str if c in '0123456789abcdef') / max(len(url_str), 1)\n",
        "\n",
        "        # 8. Character distribution features\n",
        "        features['digit_ratio'] = sum(1 for c in url_str if c.isdigit()) / max(len(url_str), 1)\n",
        "        features['letter_ratio'] = sum(1 for c in url_str if c.isalpha()) / max(len(url_str), 1)\n",
        "        features['special_char_ratio'] = sum(1 for c in url_str if not c.isalnum() and c not in ['.', '-', '_', '/']) / max(len(url_str), 1)\n",
        "        features['vowel_ratio'] = sum(1 for c in url_str if c in 'aeiou') / max(len(url_str), 1)\n",
        "\n",
        "        # 9. Specific pattern features\n",
        "        features['has_login'] = 1 if 'login' in url_str else 0\n",
        "        features['has_signin'] = 1 if 'signin' in url_str else 0\n",
        "        features['has_verify'] = 1 if 'verify' in url_str else 0\n",
        "        features['has_bank'] = 1 if 'bank' in url_str else 0\n",
        "        features['has_paypal'] = 1 if 'paypal' in url_str else 0\n",
        "        features['has_secure'] = 1 if 'secure' in url_str else 0\n",
        "\n",
        "        # 10. Entropy (measure of randomness)\n",
        "        if url_str:\n",
        "            freq = Counter(url_str)\n",
        "            prob = [float(freq[c]) / len(url_str) for c in freq]\n",
        "            features['entropy'] = -sum([p * math.log(p) / math.log(2.0) for p in prob])\n",
        "        else:\n",
        "            features['entropy'] = 0\n",
        "\n",
        "        # 11. Consecutive characters\n",
        "        features['consecutive_digits'] = max(len(match) for match in re.findall(r'\\d+', url_str)) if re.findall(r'\\d+', url_str) else 0\n",
        "        features['consecutive_chars'] = max(len(match) for match in re.findall(r'[a-z]+', url_str)) if re.findall(r'[a-z]+', url_str) else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def transform(self, urls):\n",
        "        features_list = []\n",
        "        for url in urls:\n",
        "            features = self.extract_features(url)\n",
        "            features_list.append(list(features.values()))\n",
        "\n",
        "        feature_names = list(self.extract_features(\"https://example.com\").keys())\n",
        "        return pd.DataFrame(features_list, columns=feature_names)\n",
        "\n",
        "# ========================\n",
        "# INITIALIZE NLTK COMPONENTS\n",
        "# ========================\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "# ========================\n",
        "# MAIN DETECTOR CLASS\n",
        "# ========================\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.models_loaded = False\n",
        "        self.loaded_models = {}\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "        self.model_names_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models from root directory\"\"\"\n",
        "        try:\n",
        "            print(\"üìÅ Looking for model files in root directory...\")\n",
        "\n",
        "            files = os.listdir('.')\n",
        "            print(f\"üìã Found {len(files)} files in directory\")\n",
        "\n",
        "            found_models = []\n",
        "\n",
        "            # Load pickle models\n",
        "            pickle_files = [\n",
        "                ('phishing_tfidf_vectorizer.pkl', 'tfidf_vectorizer'),\n",
        "                ('phishing_feature_extractor.pkl', 'feature_extractor'),\n",
        "                ('phishing_keras_tokenizer.pkl', 'keras_tokenizer'),\n",
        "                ('phishing_lr_model.pkl', 'lr_model'),\n",
        "                ('phishing_nb_model.pkl', 'nb_model'),\n",
        "                ('phishing_rf_model.pkl', 'rf_model'),\n",
        "                ('phishing_gb_model.pkl', 'gb_model')\n",
        "            ]\n",
        "\n",
        "            for file_name, model_name in pickle_files:\n",
        "                if file_name in files:\n",
        "                    try:\n",
        "                        with open(file_name, 'rb') as f:\n",
        "                            setattr(self, model_name, pickle.load(f))\n",
        "                            self.loaded_models[model_name] = True\n",
        "                            found_models.append(file_name)\n",
        "                            print(f\"‚úÖ Loaded {file_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file_name}: {str(e)[:100]}\")\n",
        "\n",
        "            # Load keras models\n",
        "            keras_files = [\n",
        "                ('best_cnn_model.keras', 'cnn_model'),\n",
        "                ('best_lstm_model.keras', 'lstm_model'),\n",
        "                ('best_gru_model.keras', 'gru_model'),\n",
        "                ('best_hybrid_model.keras', 'hybrid_model'),\n",
        "                ('phishing_cnn_model.keras', 'cnn_model'),\n",
        "                ('phishing_lstm_model.keras', 'lstm_model'),\n",
        "                ('phishing_gru_model.keras', 'gru_model'),\n",
        "                ('phishing_hybrid_model.keras', 'hybrid_model')\n",
        "            ]\n",
        "\n",
        "            loaded_keras = set()\n",
        "            for file_name, model_name in keras_files:\n",
        "                if file_name in files and model_name not in loaded_keras:\n",
        "                    try:\n",
        "                        setattr(self, model_name, tf.keras.models.load_model(file_name))\n",
        "                        self.loaded_models[model_name] = True\n",
        "                        loaded_keras.add(model_name)\n",
        "                        found_models.append(file_name)\n",
        "                        print(f\"‚úÖ Loaded {file_name} as {model_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file_name}: {str(e)[:100]}\")\n",
        "\n",
        "            if len(found_models) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found in current directory.\"\n",
        "\n",
        "            essential_models = ['feature_extractor', 'tfidf_vectorizer', 'keras_tokenizer']\n",
        "            missing = [m for m in essential_models if m not in self.loaded_models]\n",
        "\n",
        "            if missing:\n",
        "                print(f\"‚ö†Ô∏è Missing essential models: {missing}\")\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(found_models)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            if 'feature_extractor' not in self.loaded_models or 'tfidf_vectorizer' not in self.loaded_models:\n",
        "                return None, None, None\n",
        "\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            processed_url = self.preprocess_url(url)\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if model_attr not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if 'nb_model' not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if model_attr not in self.loaded_models or 'keras_tokenizer' not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "            else:\n",
        "                return None, None, None\n",
        "\n",
        "            return prediction, float(proba), features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction for {model_type}: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if 'feature_extractor' not in self.loaded_models or 'tfidf_vectorizer' not in self.loaded_models:\n",
        "                return None, None, None, None\n",
        "\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            processed_url = self.preprocess_url(url)\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if f'{model_name}_model' in self.loaded_models:\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(float(proba))\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            if 'nb_model' in self.loaded_models:\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(float(nb_proba))\n",
        "                model_names.append('nb')\n",
        "\n",
        "            if 'keras_tokenizer' in self.loaded_models:\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    if f'{model_name}_model' in self.loaded_models:\n",
        "                        model = getattr(self, f'{model_name}_model)\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(float(dl_proba))\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = all_probas[i]\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        full_names = [self.model_names_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nüìä Analyzing URL: {url}\")\n",
        "            print(f\"ü§ñ Using model: {model_choice}\")\n",
        "\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                return self.demo_prediction(url, model_choice), None, None, None, None, \"‚ö†Ô∏è Using demo mode\"\n",
        "\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': [self.model_names_map.get(m, m) for m in model_scores.keys()],\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analyze_url: {e}\")\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "    def demo_prediction(self, url, model_choice):\n",
        "        \"\"\"Demo prediction when models are not available\"\"\"\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        phishing_indicators = ['login', 'verify', 'secure', 'account', 'bank', 'paypal', 'update']\n",
        "        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz']\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        if not url_lower.startswith('https://'):\n",
        "            score += 0.2\n",
        "\n",
        "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_lower):\n",
        "            score += 0.3\n",
        "\n",
        "        for keyword in phishing_indicators:\n",
        "            if keyword in url_lower:\n",
        "                score += 0.1\n",
        "\n",
        "        for tld in suspicious_tlds:\n",
        "            if tld in url_lower:\n",
        "                score += 0.2\n",
        "\n",
        "        if len(url) > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        phishing_prob = min(score, 0.9)\n",
        "\n",
        "        result_text = f\"## üîç Analysis Results (DEMO MODE)\\n\\n\"\n",
        "        result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "        result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "        result_text += f\"‚ö†Ô∏è **Note:** Running in demo mode. Models not fully loaded.\\n\\n\"\n",
        "\n",
        "        if phishing_prob > 0.5:\n",
        "            result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "        else:\n",
        "            result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {(1-phishing_prob)*100:.2f}%\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "# ========================\n",
        "# CREATE GRADIO INTERFACE\n",
        "# ========================\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# ========================\n",
        "# LAUNCH THE APP\n",
        "# ========================\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "print(\"\\nüìã Models loaded successfully! Now launching interface...\")\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xYbaL7kLAztN",
        "outputId": "2345be01-a730-4a2e-fb70-8e9c0965cf53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated f-string literal (detected at line 381) (ipython-input-2339166992.py, line 381)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2339166992.py\"\u001b[0;36m, line \u001b[0;32m381\u001b[0m\n\u001b[0;31m    model = getattr(self, f'{model_name}_model)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated f-string literal (detected at line 381)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install gradio --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "\n",
        "# Download NLTK data FIRST\n",
        "import nltk\n",
        "\n",
        "# Try to download stopwords with proper error handling\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK stopwords already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=False)\n",
        "    print(\"‚úÖ NLTK stopwords downloaded\")\n",
        "\n",
        "# Now import other packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import hstack\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Import NLTK components\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ========================\n",
        "# DEFINE ENHANCEDURLFEATUREEXTRACTOR CLASS FIRST\n",
        "# ========================\n",
        "class EnhancedURLFeatureExtractor:\n",
        "    \"\"\"Extract comprehensive features from URLs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.phishing_keywords = [\n",
        "            'login', 'signin', 'verify', 'secure', 'account', 'update',\n",
        "            'banking', 'paypal', 'confirm', 'password', 'authenticate',\n",
        "            'validation', 'security', 'webscr', 'signup', 'login-secure',\n",
        "            'bank', 'credit', 'card', 'ssn', 'social', 'irs', 'tax',\n",
        "            'update', 'verify', 'wallet', 'bitcoin', 'crypto', 'wallet'\n",
        "        ]\n",
        "\n",
        "        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz',\n",
        "                                '.top', '.club', '.work', '.online', '.site']\n",
        "\n",
        "        self.shortening_services = ['bit.ly', 'tinyurl', 'goo.gl', 'shorte.st',\n",
        "                                   'ow.ly', 't.co', 'is.gd', 'cli.gs', 'yfrog.com',\n",
        "                                   'migre.me', 'ff.im', 'tiny.cc', 'url4.eu',\n",
        "                                   'twit.ac', 'su.pr', 'twurl.nl', 'snipurl.com',\n",
        "                                   'short.to', 'budurl.com', 'ping.fm', 'post.ly',\n",
        "                                   'just.as', 'bkite.com', 'snipr.com', 'fic.kr',\n",
        "                                   'loopt.us', 'doiop.com', 'short.ie', 'kl.am',\n",
        "                                   'wp.me', 'rubyurl.com', 'om.ly', 'to.ly',\n",
        "                                   'bit.do', 't.co', 'lnkd.in', 'db.tt', 'qr.ae',\n",
        "                                   'adf.ly', 'goo.gl', 'bitly.com', 'cur.lv',\n",
        "                                   'tinyurl.com', 'ow.ly', 'bit.ly', 'ity.im',\n",
        "                                   'q.gs', 'is.gd', 'po.st', 'bc.vc', 'twitthis.com',\n",
        "                                   'u.to', 'j.mp', 'buzurl.com', 'cutt.us',\n",
        "                                   'u.bb', 'yourls.org', 'x.co', 'prettylinkpro.com',\n",
        "                                   'scrnch.me', 'filoops.info', 'vzturl.com',\n",
        "                                   'qr.net', '1url.com', 'tweez.me', 'v.gd',\n",
        "                                   'tr.im', 'link.zip.net']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        features = {}\n",
        "\n",
        "        # URL string\n",
        "        url_str = str(url).lower()\n",
        "\n",
        "        # 1. Length-based features\n",
        "        features['url_length'] = len(url_str)\n",
        "        features['hostname_length'] = len(url_str.split('//')[-1].split('/')[0]) if '//' in url_str else len(url_str.split('/')[0])\n",
        "        features['path_length'] = len('/'.join(url_str.split('/')[3:]))\n",
        "        features['num_dots'] = url_str.count('.')\n",
        "        features['num_hyphens'] = url_str.count('-')\n",
        "        features['num_underscores'] = url_str.count('_')\n",
        "        features['num_slashes'] = url_str.count('/')\n",
        "        features['num_questionmarks'] = url_str.count('?')\n",
        "        features['num_equals'] = url_str.count('=')\n",
        "        features['num_ats'] = url_str.count('@')\n",
        "        features['num_ampersands'] = url_str.count('&')\n",
        "        features['num_percent'] = url_str.count('%')\n",
        "\n",
        "        # 2. Protocol features\n",
        "        features['has_https'] = 1 if url_str.startswith('https://') else 0\n",
        "        features['has_http'] = 1 if url_str.startswith('http://') else 0\n",
        "\n",
        "        # 3. Domain features\n",
        "        if '//' in url_str:\n",
        "            domain_part = url_str.split('//')[1].split('/')[0]\n",
        "        else:\n",
        "            domain_part = url_str.split('/')[0]\n",
        "\n",
        "        features['domain_length'] = len(domain_part)\n",
        "        features['num_subdomains'] = domain_part.count('.') - 1 if '.' in domain_part else 0\n",
        "\n",
        "        # 4. TLD features\n",
        "        tld = domain_part.split('.')[-1] if '.' in domain_part else ''\n",
        "        features['has_suspicious_tld'] = 1 if any(suspicious_tld in url_str for suspicious_tld in self.suspicious_tlds) else 0\n",
        "        features['tld_length'] = len(tld)\n",
        "\n",
        "        # 5. URL shortening detection\n",
        "        features['is_shortened'] = 1 if any(short in domain_part for short in self.shortening_services) else 0\n",
        "\n",
        "        # 6. Keyword features\n",
        "        keyword_count = 0\n",
        "        for keyword in self.phishing_keywords:\n",
        "            if keyword in url_str:\n",
        "                keyword_count += 1\n",
        "\n",
        "        features['phishing_keyword_count'] = keyword_count\n",
        "        features['has_phishing_keyword'] = 1 if keyword_count > 0 else 0\n",
        "\n",
        "        # 7. Suspicious patterns\n",
        "        features['has_ip'] = 1 if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_str) else 0\n",
        "        features['hex_chars_ratio'] = sum(1 for c in url_str if c in '0123456789abcdef') / max(len(url_str), 1)\n",
        "\n",
        "        # 8. Character distribution features\n",
        "        features['digit_ratio'] = sum(1 for c in url_str if c.isdigit()) / max(len(url_str), 1)\n",
        "        features['letter_ratio'] = sum(1 for c in url_str if c.isalpha()) / max(len(url_str), 1)\n",
        "        features['special_char_ratio'] = sum(1 for c in url_str if not c.isalnum() and c not in ['.', '-', '_', '/']) / max(len(url_str), 1)\n",
        "        features['vowel_ratio'] = sum(1 for c in url_str if c in 'aeiou') / max(len(url_str), 1)\n",
        "\n",
        "        # 9. Specific pattern features\n",
        "        features['has_login'] = 1 if 'login' in url_str else 0\n",
        "        features['has_signin'] = 1 if 'signin' in url_str else 0\n",
        "        features['has_verify'] = 1 if 'verify' in url_str else 0\n",
        "        features['has_bank'] = 1 if 'bank' in url_str else 0\n",
        "        features['has_paypal'] = 1 if 'paypal' in url_str else 0\n",
        "        features['has_secure'] = 1 if 'secure' in url_str else 0\n",
        "\n",
        "        # 10. Entropy (measure of randomness)\n",
        "        if url_str:\n",
        "            freq = Counter(url_str)\n",
        "            prob = [float(freq[c]) / len(url_str) for c in freq]\n",
        "            features['entropy'] = -sum([p * math.log(p) / math.log(2.0) for p in prob])\n",
        "        else:\n",
        "            features['entropy'] = 0\n",
        "\n",
        "        # 11. Consecutive characters\n",
        "        features['consecutive_digits'] = max(len(match) for match in re.findall(r'\\d+', url_str)) if re.findall(r'\\d+', url_str) else 0\n",
        "        features['consecutive_chars'] = max(len(match) for match in re.findall(r'[a-z]+', url_str)) if re.findall(r'[a-z]+', url_str) else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def transform(self, urls):\n",
        "        features_list = []\n",
        "        for url in urls:\n",
        "            features = self.extract_features(url)\n",
        "            features_list.append(list(features.values()))\n",
        "\n",
        "        feature_names = list(self.extract_features(\"https://example.com\").keys())\n",
        "        return pd.DataFrame(features_list, columns=feature_names)\n",
        "\n",
        "# ========================\n",
        "# INITIALIZE NLTK COMPONENTS\n",
        "# ========================\n",
        "try:\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    print(\"‚úÖ NLTK components initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error initializing NLTK: {e}\")\n",
        "    tokenizer_nltk = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set()\n",
        "\n",
        "# ========================\n",
        "# MAIN DETECTOR CLASS\n",
        "# ========================\n",
        "class PhishingURLDetectorUI:\n",
        "    def __init__(self):\n",
        "        self.models_loaded = False\n",
        "        self.loaded_models = {}\n",
        "        self.model_accuracies = {\n",
        "            'Logistic Regression': 0.9960,\n",
        "            'Naive Bayes': 0.9890,\n",
        "            'Random Forest': 0.9963,\n",
        "            'Gradient Boosting': 0.9977,\n",
        "            'CNN': 0.99805,\n",
        "            'LSTM': 0.99785,\n",
        "            'GRU': 0.99765,\n",
        "            'Hybrid CNN-RNN': 0.99800,\n",
        "            'Ensemble': 0.99805\n",
        "        }\n",
        "        self.model_names_map = {\n",
        "            'lr': 'Logistic Regression',\n",
        "            'rf': 'Random Forest',\n",
        "            'gb': 'Gradient Boosting',\n",
        "            'nb': 'Naive Bayes',\n",
        "            'cnn': 'CNN',\n",
        "            'lstm': 'LSTM',\n",
        "            'gru': 'GRU',\n",
        "            'hybrid': 'Hybrid CNN-RNN'\n",
        "        }\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all saved models from root directory\"\"\"\n",
        "        try:\n",
        "            print(\"üìÅ Looking for model files in root directory...\")\n",
        "\n",
        "            files = os.listdir('.')\n",
        "            print(f\"üìã Found {len(files)} files in directory\")\n",
        "\n",
        "            found_models = []\n",
        "\n",
        "            # Load pickle models\n",
        "            pickle_files = [\n",
        "                ('phishing_tfidf_vectorizer.pkl', 'tfidf_vectorizer'),\n",
        "                ('phishing_feature_extractor.pkl', 'feature_extractor'),\n",
        "                ('phishing_keras_tokenizer.pkl', 'keras_tokenizer'),\n",
        "                ('phishing_lr_model.pkl', 'lr_model'),\n",
        "                ('phishing_nb_model.pkl', 'nb_model'),\n",
        "                ('phishing_rf_model.pkl', 'rf_model'),\n",
        "                ('phishing_gb_model.pkl', 'gb_model')\n",
        "            ]\n",
        "\n",
        "            for file_name, model_name in pickle_files:\n",
        "                if file_name in files:\n",
        "                    try:\n",
        "                        with open(file_name, 'rb') as f:\n",
        "                            setattr(self, model_name, pickle.load(f))\n",
        "                            self.loaded_models[model_name] = True\n",
        "                            found_models.append(file_name)\n",
        "                            print(f\"‚úÖ Loaded {file_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file_name}: {str(e)[:100]}\")\n",
        "\n",
        "            # Load keras models\n",
        "            keras_files = [\n",
        "                ('best_cnn_model.keras', 'cnn_model'),\n",
        "                ('best_lstm_model.keras', 'lstm_model'),\n",
        "                ('best_gru_model.keras', 'gru_model'),\n",
        "                ('best_hybrid_model.keras', 'hybrid_model'),\n",
        "                ('phishing_cnn_model.keras', 'cnn_model'),\n",
        "                ('phishing_lstm_model.keras', 'lstm_model'),\n",
        "                ('phishing_gru_model.keras', 'gru_model'),\n",
        "                ('phishing_hybrid_model.keras', 'hybrid_model')\n",
        "            ]\n",
        "\n",
        "            loaded_keras = set()\n",
        "            for file_name, model_name in keras_files:\n",
        "                if file_name in files and model_name not in loaded_keras:\n",
        "                    try:\n",
        "                        setattr(self, model_name, tf.keras.models.load_model(file_name))\n",
        "                        self.loaded_models[model_name] = True\n",
        "                        loaded_keras.add(model_name)\n",
        "                        found_models.append(file_name)\n",
        "                        print(f\"‚úÖ Loaded {file_name} as {model_name}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è Error loading {file_name}: {str(e)[:100]}\")\n",
        "\n",
        "            if len(found_models) == 0:\n",
        "                return \"‚ö†Ô∏è No model files found in current directory.\"\n",
        "\n",
        "            essential_models = ['feature_extractor', 'tfidf_vectorizer', 'keras_tokenizer']\n",
        "            missing = [m for m in essential_models if m not in self.loaded_models]\n",
        "\n",
        "            if missing:\n",
        "                print(f\"‚ö†Ô∏è Missing essential models: {missing}\")\n",
        "\n",
        "            self.models_loaded = True\n",
        "            return f\"‚úÖ Successfully loaded {len(found_models)} model files!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error loading models: {str(e)}\"\n",
        "\n",
        "    def preprocess_url(self, url):\n",
        "        \"\"\"Preprocess URL text\"\"\"\n",
        "        url_str = str(url).lower()\n",
        "        try:\n",
        "            tokens = tokenizer_nltk.tokenize(url_str)\n",
        "            tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
        "        except:\n",
        "            tokens = re.findall(r'[a-z]+', url_str)\n",
        "            tokens = [t for t in tokens if len(t) > 2]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def predict_single_model(self, url, model_type):\n",
        "        \"\"\"Predict using a single model\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None\n",
        "\n",
        "        try:\n",
        "            if 'feature_extractor' not in self.loaded_models or 'tfidf_vectorizer' not in self.loaded_models:\n",
        "                return None, None, None\n",
        "\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            processed_url = self.preprocess_url(url)\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            if model_type in ['lr', 'rf', 'gb']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if model_attr not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict_proba(features_combined)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type == 'nb':\n",
        "                if 'nb_model' not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "\n",
        "            elif model_type in ['cnn', 'lstm', 'gru', 'hybrid']:\n",
        "                model_attr = f'{model_type}_model'\n",
        "                if model_attr not in self.loaded_models or 'keras_tokenizer' not in self.loaded_models:\n",
        "                    return None, None, None\n",
        "\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "                model = getattr(self, model_attr)\n",
        "                proba = model.predict(padded, verbose=0)[0][0]\n",
        "                prediction = 1 if proba > 0.5 else 0\n",
        "            else:\n",
        "                return None, None, None\n",
        "\n",
        "            return prediction, float(proba), features_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction for {model_type}: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def predict_ensemble(self, url):\n",
        "        \"\"\"Predict using ensemble of all models\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if 'feature_extractor' not in self.loaded_models or 'tfidf_vectorizer' not in self.loaded_models:\n",
        "                return None, None, None, None\n",
        "\n",
        "            handcrafted_features = self.feature_extractor.transform([url])\n",
        "            features_dict = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            processed_url = self.preprocess_url(url)\n",
        "            tfidf_features = self.tfidf_vectorizer.transform([processed_url])\n",
        "\n",
        "            all_probas = []\n",
        "            model_names = []\n",
        "\n",
        "            features_combined = hstack([tfidf_features, handcrafted_features.values])\n",
        "\n",
        "            ml_models = ['lr', 'rf', 'gb']\n",
        "            for model_name in ml_models:\n",
        "                if f'{model_name}_model' in self.loaded_models:\n",
        "                    model = getattr(self, f'{model_name}_model')\n",
        "                    proba = model.predict_proba(features_combined)[0][1]\n",
        "                    all_probas.append(float(proba))\n",
        "                    model_names.append(model_name)\n",
        "\n",
        "            if 'nb_model' in self.loaded_models:\n",
        "                nb_proba = self.nb_model.predict_proba(tfidf_features)[0][1]\n",
        "                all_probas.append(float(nb_proba))\n",
        "                model_names.append('nb')\n",
        "\n",
        "            if 'keras_tokenizer' in self.loaded_models:\n",
        "                seq = self.keras_tokenizer.texts_to_sequences([url])\n",
        "                padded = pad_sequences(seq, maxlen=200, padding='post')\n",
        "\n",
        "                dl_models = ['cnn', 'lstm', 'gru', 'hybrid']\n",
        "                for model_name in dl_models:\n",
        "                    if f'{model_name}_model' in self.loaded_models:\n",
        "                        model = getattr(self, f'{model_name}_model')\n",
        "                        dl_proba = model.predict(padded, verbose=0)[0][0]\n",
        "                        all_probas.append(float(dl_proba))\n",
        "                        model_names.append(model_name)\n",
        "\n",
        "            if not all_probas:\n",
        "                return None, None, None, None\n",
        "\n",
        "            ensemble_proba = np.mean(all_probas)\n",
        "            prediction = 1 if ensemble_proba > 0.5 else 0\n",
        "\n",
        "            model_scores = {}\n",
        "            for i, name in enumerate(model_names):\n",
        "                model_scores[name] = all_probas[i]\n",
        "\n",
        "            return prediction, ensemble_proba, features_dict, model_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble prediction: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def create_prediction_plot(self, phishing_prob, legitimate_prob):\n",
        "        \"\"\"Create a bar plot for prediction probabilities\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        categories = ['Phishing', 'Legitimate']\n",
        "        probabilities = [phishing_prob * 100, legitimate_prob * 100]\n",
        "        colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "        bars = ax.bar(categories, probabilities, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "        for bar, prob in zip(bars, probabilities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                   f'{prob:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "        ax.set_ylabel('Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('URL Classification Results', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_ylim(0, 105)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_accuracy_chart(self):\n",
        "        \"\"\"Create a bar chart showing model accuracies\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        models = list(self.model_accuracies.keys())\n",
        "        accuracies = [self.model_accuracies[m] * 100 for m in models]\n",
        "\n",
        "        colors = cm.viridis(np.linspace(0.3, 0.9, len(models)))\n",
        "\n",
        "        bars = ax.barh(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{acc:.2f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_scores_chart(self, model_scores):\n",
        "        \"\"\"Create a bar chart for individual model scores\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        full_names = [self.model_names_map.get(m, m) for m in model_scores.keys()]\n",
        "        scores = [v * 100 for v in model_scores.values()]\n",
        "\n",
        "        colors = ['#ff6b6b' if score > 50 else '#51cf66' for score in scores]\n",
        "\n",
        "        bars = ax.barh(full_names, scores, color=colors, edgecolor='black', linewidth=1)\n",
        "\n",
        "        for bar, score in zip(bars, scores):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{score:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Phishing Probability (%)', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Individual Model Predictions', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.set_xlim(0, 105)\n",
        "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "        ax.axvline(x=50, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
        "        ax.text(50, len(full_names) - 0.5, 'Decision Threshold (50%)',\n",
        "                rotation=90, va='bottom', ha='right', backgroundcolor='white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def analyze_url(self, url, model_choice):\n",
        "        \"\"\"Main analysis function for Gradio\"\"\"\n",
        "        if not url or url.strip() == \"\":\n",
        "            return \"Please enter a URL to analyze.\", None, None, None, None, None\n",
        "\n",
        "        if not self.models_loaded:\n",
        "            load_status = self.load_models()\n",
        "            if \"Error\" in load_status or \"‚ö†Ô∏è\" in load_status:\n",
        "                return load_status, None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nüìä Analyzing URL: {url}\")\n",
        "            print(f\"ü§ñ Using model: {model_choice}\")\n",
        "\n",
        "            if model_choice == \"Ensemble (All Models)\":\n",
        "                prediction, proba, features, model_scores = self.predict_ensemble(url)\n",
        "            else:\n",
        "                model_map = {\n",
        "                    \"Logistic Regression\": \"lr\",\n",
        "                    \"Naive Bayes\": \"nb\",\n",
        "                    \"Random Forest\": \"rf\",\n",
        "                    \"Gradient Boosting\": \"gb\",\n",
        "                    \"CNN\": \"cnn\",\n",
        "                    \"LSTM\": \"lstm\",\n",
        "                    \"GRU\": \"gru\",\n",
        "                    \"Hybrid CNN-RNN\": \"hybrid\"\n",
        "                }\n",
        "                model_type = model_map.get(model_choice, \"lr\")\n",
        "                prediction, proba, features = self.predict_single_model(url, model_type)\n",
        "                model_scores = None\n",
        "\n",
        "            if prediction is None:\n",
        "                return self.demo_prediction(url, model_choice), None, None, None, None, \"‚ö†Ô∏è Using demo mode\"\n",
        "\n",
        "            phishing_prob = proba if prediction == 1 else 1 - proba\n",
        "            legitimate_prob = 1 - phishing_prob\n",
        "\n",
        "            result_text = f\"## üîç Analysis Results\\n\\n\"\n",
        "            result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "            result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "\n",
        "            if prediction == 1:\n",
        "                result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "                result_text += f\"**Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "            else:\n",
        "                result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "                result_text += f\"**Confidence:** {legitimate_prob*100:.2f}%\\n\"\n",
        "\n",
        "            if features:\n",
        "                result_text += f\"\\n**Key Features:**\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Length: {features.get('url_length', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has HTTPS: {'‚úÖ Yes' if features.get('has_https', 0) == 1 else '‚ùå No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Has IP Address: {'‚ö†Ô∏è Yes' if features.get('has_ip', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Phishing Keywords: {features.get('phishing_keyword_count', 0)}\\n\"\n",
        "                result_text += f\"‚Ä¢ Suspicious TLD: {'‚ö†Ô∏è Yes' if features.get('has_suspicious_tld', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ URL Shortener: {'‚ö†Ô∏è Yes' if features.get('is_shortened', 0) == 1 else '‚úÖ No'}\\n\"\n",
        "                result_text += f\"‚Ä¢ Entropy: {features.get('entropy', 0):.3f}\\n\"\n",
        "\n",
        "            plot1 = self.create_prediction_plot(phishing_prob, legitimate_prob)\n",
        "\n",
        "            if model_scores and model_choice == \"Ensemble (All Models)\":\n",
        "                plot2 = self.create_model_scores_chart(model_scores)\n",
        "                scores_df = pd.DataFrame({\n",
        "                    'Model': [self.model_names_map.get(m, m) for m in model_scores.keys()],\n",
        "                    'Phishing Probability': [f\"{v*100:.1f}%\" for v in model_scores.values()]\n",
        "                })\n",
        "                scores_table = scores_df.to_markdown(index=False)\n",
        "            else:\n",
        "                plot2 = self.create_model_accuracy_chart()\n",
        "                scores_table = \"\"\n",
        "\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Metric': ['Phishing Probability', 'Legitimate Probability', 'Confidence'],\n",
        "                'Value': [f\"{phishing_prob*100:.2f}%\", f\"{legitimate_prob*100:.2f}%\",\n",
        "                         f\"{max(phishing_prob, legitimate_prob)*100:.2f}%\"]\n",
        "            })\n",
        "\n",
        "            return result_text, plot1, plot2, metrics_df.to_markdown(index=False), scores_table, \"‚úÖ Analysis Complete\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analyze_url: {e}\")\n",
        "            return f\"Error analyzing URL: {str(e)}\", None, None, None, None, \"‚ùå Analysis Failed\"\n",
        "\n",
        "    def demo_prediction(self, url, model_choice):\n",
        "        \"\"\"Demo prediction when models are not available\"\"\"\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        phishing_indicators = ['login', 'verify', 'secure', 'account', 'bank', 'paypal', 'update']\n",
        "        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz']\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        if not url_lower.startswith('https://'):\n",
        "            score += 0.2\n",
        "\n",
        "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url_lower):\n",
        "            score += 0.3\n",
        "\n",
        "        for keyword in phishing_indicators:\n",
        "            if keyword in url_lower:\n",
        "                score += 0.1\n",
        "\n",
        "        for tld in suspicious_tlds:\n",
        "            if tld in url_lower:\n",
        "                score += 0.2\n",
        "\n",
        "        if len(url) > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        phishing_prob = min(score, 0.9)\n",
        "\n",
        "        result_text = f\"## üîç Analysis Results (DEMO MODE)\\n\\n\"\n",
        "        result_text += f\"**URL:** `{url}`\\n\\n\"\n",
        "        result_text += f\"**Model Used:** {model_choice}\\n\\n\"\n",
        "        result_text += f\"‚ö†Ô∏è **Note:** Running in demo mode. Models not fully loaded.\\n\\n\"\n",
        "\n",
        "        if phishing_prob > 0.5:\n",
        "            result_text += f\"**Prediction:** üî¥ **PHISHING** (High Risk)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {phishing_prob*100:.2f}%\\n\"\n",
        "        else:\n",
        "            result_text += f\"**Prediction:** üü¢ **LEGITIMATE** (Safe)\\n\"\n",
        "            result_text += f\"**Demo Confidence:** {(1-phishing_prob)*100:.2f}%\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "# ========================\n",
        "# CREATE GRADIO INTERFACE\n",
        "# ========================\n",
        "detector_ui = PhishingURLDetectorUI()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîó Phishing URL Detection System\n",
        "    ### Advanced ML/DL models to detect malicious URLs with high accuracy\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### üìù Enter URL to Analyze\")\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"URL\",\n",
        "                placeholder=\"https://example.com\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Select Detection Model\")\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"Ensemble (All Models)\",\n",
        "                    \"Logistic Regression\",\n",
        "                    \"Naive Bayes\",\n",
        "                    \"Random Forest\",\n",
        "                    \"Gradient Boosting\",\n",
        "                    \"CNN\",\n",
        "                    \"LSTM\",\n",
        "                    \"GRU\",\n",
        "                    \"Hybrid CNN-RNN\"\n",
        "                ],\n",
        "                value=\"Ensemble (All Models)\",\n",
        "                label=\"Model Selection\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Analyze URL\", variant=\"primary\", size=\"lg\")\n",
        "            status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìä Model Accuracies\")\n",
        "            accuracy_plot = gr.Plot(label=\"Model Performance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üìà Analysis Results\")\n",
        "            result_output = gr.Markdown(label=\"Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìä Prediction Probabilities\")\n",
        "                    prediction_plot = gr.Plot(label=\"Classification Results\")\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### üìã Detailed Metrics\")\n",
        "                    metrics_table = gr.Markdown(label=\"Metrics\")\n",
        "\n",
        "            gr.Markdown(\"### ü§ñ Model Predictions\")\n",
        "            scores_table = gr.Markdown(label=\"Individual Model Scores\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí° Example URLs to Test\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"https://secure-login-paypal.com/verify-account\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://www.google.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"https://github.com\", \"CNN\"],\n",
        "                    [\"http://192.168.1.100/login.php\", \"Random Forest\"],\n",
        "                    [\"https://www.amazon.com\", \"Ensemble (All Models)\"],\n",
        "                    [\"http://update-your-banking-info-now.xyz\", \"Hybrid CNN-RNN\"]\n",
        "                ],\n",
        "                inputs=[url_input, model_choice],\n",
        "                label=\"Try these examples\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è Safety Tips\n",
        "            1. **Check HTTPS**: Always look for the padlock icon\n",
        "            2. **Verify Domain**: Check for misspellings in domain names\n",
        "            3. **Avoid Short URLs**: Be cautious of shortened URLs\n",
        "            4. **Check for IPs**: URLs with IP addresses are suspicious\n",
        "            5. **Look for Keywords**: Phishing URLs often contain 'login', 'verify', 'secure'\n",
        "            \"\"\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Phishing URL Detection System** | Using Advanced Machine Learning & Deep Learning Models\n",
        "    ‚ö†Ô∏è *This tool is for educational purposes. Always verify suspicious URLs through official channels.*\n",
        "    \"\"\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        fn=detector_ui.analyze_url,\n",
        "        inputs=[url_input, model_choice],\n",
        "        outputs=[result_output, prediction_plot, accuracy_plot, metrics_table, scores_table, status_text]\n",
        "    )\n",
        "\n",
        "    def initialize():\n",
        "        return detector_ui.create_model_accuracy_chart()\n",
        "\n",
        "    demo.load(initialize, outputs=[accuracy_plot])\n",
        "\n",
        "# ========================\n",
        "# LAUNCH THE APP\n",
        "# ========================\n",
        "print(\"üöÄ Launching Phishing URL Detection UI...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "load_status = detector_ui.load_models()\n",
        "print(load_status)\n",
        "\n",
        "print(\"\\nüìã Models loaded successfully! Now launching interface...\")\n",
        "print(\"\\nüåê Launching Gradio interface...\")\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OR73jWASC2pC",
        "outputId": "92f4272a-0326-4c14-b950-fd66db3c81a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK stopwords already downloaded\n",
            "‚úÖ NLTK components initialized successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3087003444.py:607: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Phishing URL Detector\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Phishing URL Detection UI...\n",
            "============================================================\n",
            "üìÅ Looking for model files in root directory...\n",
            "üìã Found 20 files in directory\n",
            "‚úÖ Loaded phishing_tfidf_vectorizer.pkl\n",
            "‚úÖ Loaded phishing_feature_extractor.pkl\n",
            "‚úÖ Loaded phishing_keras_tokenizer.pkl\n",
            "‚úÖ Loaded phishing_lr_model.pkl\n",
            "‚úÖ Loaded phishing_nb_model.pkl\n",
            "‚úÖ Loaded phishing_rf_model.pkl\n",
            "‚úÖ Loaded phishing_gb_model.pkl\n",
            "‚úÖ Loaded best_cnn_model.keras as cnn_model\n",
            "‚úÖ Loaded best_lstm_model.keras as lstm_model\n",
            "‚úÖ Loaded best_gru_model.keras as gru_model\n",
            "‚úÖ Loaded best_hybrid_model.keras as hybrid_model\n",
            "‚úÖ Successfully loaded 11 model files!\n",
            "\n",
            "üìã Models loaded successfully! Now launching interface...\n",
            "\n",
            "üåê Launching Gradio interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f69cc2d961102f18f6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f69cc2d961102f18f6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Analyzing URL: google.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: google.com\n",
            "ü§ñ Using model: LSTM\n",
            "\n",
            "üìä Analyzing URL: https://www.google.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: https://uniquewriters.unaux.com/Portal/\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: https://post-nouvellesvitales.offremanagement-acce...\n",
            "ü§ñ Using model: Ensemble (All Models)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3087003444.py:404: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, ax = plt.subplots(figsize=(8, 5))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Analyzing URL: https://www.google.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: https://post-nouvellesvitales.offremanagement-acce...\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: https://uniquewriters.unaux.com/Portal/\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: https://radar.cloudflare.com/domains/domain/google.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: instagram.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "\n",
            "üìä Analyzing URL: instagram.com\n",
            "ü§ñ Using model: Ensemble (All Models)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://f69cc2d961102f18f6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}